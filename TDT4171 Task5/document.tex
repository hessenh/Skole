% http://www.cse.iitk.ac.in/users/braman/students/good-report.html
\documentclass[english,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fullpage}
\usepackage{parskip}
\usepackage{newclude}
\usepackage{hyperref}
\usepackage{makeidx}
\usepackage{longtable}
\usepackage{multirow}

\hypersetup{
    %bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat's bookmarks
    pdftoolbar=true,        % show Acrobat's toolbar?
    pdfmenubar=true,        % show Acrobat's menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={TDT4171 Artificial Intelligence Methods - Assignment 5},
    % title
    pdfauthor={Stian Hvatum},     % author
    pdfsubject={TDT4171 Artificial Intelligence Methods},   % subject of the document
    pdfcreator={Stian Hvatum},   % creator of the document
    pdfproducer={Stian Hvatum}, % producer of the document
    pdfnewwindow=true,      % links in new window
    colorlinks,       % false: boxed links; true: colored links
    linkcolor=black,          % color of internal links
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}
\makeindex

\title{TDT4171 Artificial Intelligence Methods\\
\Small Exercise 5\\
\Huge Understanding Weka}
\author{Stian Hvatum (hvatum)\\MTDT}

\begin{document}
\maketitle
\thispagestyle{empty}
\newpage
\pagenumbering{Roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Getting familiar with Weka}
\subsection{The output}
\subsubsection{The IB1 classifier}

\begin{longtable}{|lrr|l|}
\hline
\textbf{Output} &&& \textbf{Explanation}\\
\hline
IB1 classifier	&&& \multirow{2}{210pt}{The first line in the output represents
what algorithm that was used, here the IB1 classifier.}
\\
&&&\\
\hline
\multicolumn{3}{|l|}{Time taken to build model: \emph{x} seconds}& How long time
was used for building the model.\\
\hline
\multicolumn{3}{|l|}{Time taken to test model on training data:  \emph{y}
seconds}& How long time was used for testing the model.\\
\hline
\multicolumn{3}{|l|}{=== Error on training data ===}&
This block tells us how well the training went.\\
&&&\\
Correctly Classified Instances&          14 &             100     
\%&\multirow{3}{210pt}{ The two first lines tells us how many of the lines in
the training set was correctly mapped and matched to the internal data
structure.
}\\
&&&\\
Incorrectly Classified Instances&         0 &               0      \%&\\
&&&\\
Kappa statistic                  &        1 &    &\multirow{4}{210pt}{
The Kappa statistic is a measure of how much the rows agrees to the
created model, this number is lowered once some rows disagrees with the
classification model.}
\\
&&&\\
&&&\\
&&&\\
&&&\\
Mean absolute error              &        0  &   &\multirow{3}{210pt}{
The mean absolute error is the average of the absolute error, which is the mean
distance from the correct answer that the model calculates.}\\
&&&\\
&&&\\
&&&\\
Root mean squared error          &        0  &   &\multirow{3}{210pt}{
The root mean squared is another measure of how inaccurate the model is. Lower
numbers is better.}\\
&&&\\
&&&\\
&&&\\
Relative absolute error          &&        0      \%&\multirow{2}{210pt}{
Relative absolute error tells us how good the approximation is. Lower numbers
are better. }\\
&&&\\
&&&\\
Root relative squared error      &&        0      \%&\multirow{3}{210pt}{
Root relative squared error is yet another number on how statistically correct
the calculated model is. }\\
&&&\\
&&&\\
&&&\\
Total Number of Instances        &       14  &   &\multirow{2}{210pt}{
Total Number of Instances is simply how many records that where included in the
training set. }\\
&&&\\
&&&\\
\hline
=== Confusion Matrix ===&&&\multirow{5}{210pt}{
This matrix shows us how the records where classified (top-down) and what class
they really belongs to (left-right).}\\
&&&\\
 a b   $<$- - classified as&&&\\
 9 0 $|$ a = yes&&&\\
 0 5 $|$ b = no&&&\\
 \hline
\multicolumn{3}{|l|}{=== Stratified cross-validation
===}&\multirow{10}{210pt}{These values all have the same meaning as the
above, but are in the context of test data, not training data, and thus have other
numbers attached to themseves. We see that the numbers are not as good as
with the training data. This is because the the model is build on the training
data, while the test data is new to this model. }\\
&&&\\
Correctly Classified Instances          &11       &       78.5714\%&\\
Incorrectly Classified Instances        &3        &      21.4286 \%&\\
Kappa statistic                         &0.5532&&\\
Mean absolute error                     &0.2143&&\\
Root mean squared error                 &0.4629&&\\
Relative absolute error                &&45      \%&\\
Root relative squared error            &&93.8273 \%&\\
Total Number of Instances              &14     &&\\
\hline
\end{longtable}

\subsubsection{The Id3 decision tree builder algorithm}
Much of the output is equal to the IB1 classifier-method. I have only included
those fields that are extra or different.
\begin{longtable}{|lrr|l|}
\hline
outlook = sunny&&&\\
|  humidity = high: no&&&\\
|  humidity = normal: yes&&&\\
outlook = overcast: yes&&&\\
outlook = rainy&&&\\
|  windy = TRUE: no&&&\\
|  windy = FALSE: yes&&&\\
\hline
K\&B Relative Info Score&&               1380.6963 \%&\multirow{3}{210pt}{

}\\
&&&\\
&&&\\
&&&\\
K\&B Information Score &                  13.1778 bits&      0.9413
bits/instance&\multirow{3}{210pt}{

}\\
&&&\\
&&&\\
&&&\\
Class complexity | order 0   &           13.1778 bits  &    0.9413
bits/instance&\multirow{3}{210pt}{

}\\
&&&\\
&&&\\
&&&\\
Class complexity | scheme             &   0      bits   &   0     
bits/instance&\multirow{3}{210pt}{

}\\
&&&\\
&&&\\
&&&\\
Complexity improvement     (Sf)     &    13.1778 bits  &    0.9413
bits/instance&\multirow{3}{210pt}{

}\\
&&&\\
&&&\\
&&&\\

\hline
\end{longtable}

\subsection{Different algorithms}
\subsubsection{java weka.classifiers.lazy.IB1 -t data/weather.arff}
\subsubsection{java weka.classifiers.trees.Id3 -k -t data/weather.nominal.arff}
\subsection{The .arff data format}


\end{document}
